import time
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

#MODEL_PATH = r"C:\Users\user\Desktop\qwen_sft\merged_qwen25-3b-med"
MODEL_PATH = "/models/merged_qwen25-3b-med"

_tokenizer = None
_model = None

# í”„ë¡¬í”„íŠ¸/ìƒì„± ê¸¸ì´ ì œí•œ (ì†ë„ì— ì§ì ‘ ì˜í–¥)
MAX_INSTRUCTION_CHARS = 3000    # instructionì´ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ í† í° ìˆ˜ ì¤„ì´ê¸°
MAX_PROMPT_TOKENS = 768         # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ìµœëŒ€ í† í° ìˆ˜
MAX_NEW_TOKENS = 256            # ìƒì„± ê¸¸ì´ (ê¸°ì¡´ 192 â†’ 96ìœ¼ë¡œ ì¤„ì—¬ì„œ ì†ë„ ê°œì„ )

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

def _load_model():
    global _tokenizer, _model

    if _model is not None and _tokenizer is not None:
        return _tokenizer, _model

    print("Qwen ë³‘í•© ëª¨ë¸ ë¡œë“œ ì¤‘...")
    t0 = time.time()

    _tokenizer = AutoTokenizer.from_pretrained(
        MODEL_PATH,
        trust_remote_code=True,
        local_files_only=True,   # â† ì¶”ê°€
    )

    if _tokenizer.pad_token is None:
        _tokenizer.pad_token = _tokenizer.eos_token

    if DEVICE == "cuda":
        _model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            dtype=torch.float16,
            trust_remote_code=True,
            local_files_only=True,  # â† ì¶”ê°€
        ).to(DEVICE)
    else:
        _model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            dtype=torch.float32,
            trust_remote_code=True,
            local_files_only=True,  # â† ì¶”ê°€
        )

    _model.eval()

    try:
        any_param = next(_model.parameters())
        print(f"Qwen device: {any_param.device}, dtype: {any_param.dtype}")
    except StopIteration:
        print("Qwen device: <no parameters?>")

    print(f"Qwen ë¡œë“œ ì™„ë£Œ, elapsed={time.time() - t0:.2f}s")
    return _tokenizer, _model



def _build_alpaca_prompt(instruction: str) -> str:
    """
    Alpaca ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸ ë˜í•‘.
    instruction ì•ˆì— [ì°¸ê³  ë¬¸ì„œ], [ì§ˆë¬¸], [ì§€ì‹œ]ê¹Œì§€ ëª¨ë‘ í¬í•¨ë˜ì–´ ë“¤ì–´ì˜¨ë‹¤ê³  ê°€ì •.
    """
    # instructionì´ ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ ì‚¬ìš©í•´ì„œ í† í° ìˆ˜ ì œí•œ
    if len(instruction) > MAX_INSTRUCTION_CHARS:
        instruction = instruction[:MAX_INSTRUCTION_CHARS]

    return f"""Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Response:
"""

def generate_answer(instruction: str) -> str:
    try:
        tokenizer, model = _load_model()

        alpaca_prompt = _build_alpaca_prompt(instruction)

        inputs = tokenizer(
            alpaca_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=MAX_PROMPT_TOKENS,
        )
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

        input_len = inputs["input_ids"].shape[1]

        t0 = time.time()
        with torch.inference_mode():
            outputs = model.generate(
                **inputs,
                max_new_tokens=MAX_NEW_TOKENS,
                do_sample=False,
                num_beams=1,
                use_cache=True,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.pad_token_id,
            )

        gen_elapsed = time.time() - t0
        print(f"[LLM] generate elapsed={gen_elapsed:.2f}s")

        generated_ids = outputs[0][input_len:]
        return tokenizer.decode(generated_ids, skip_special_tokens=True).strip()

    except Exception as e:
        print("ğŸ”¥ğŸ”¥ğŸ”¥ LLM ERROR OCCURRED ğŸ”¥ğŸ”¥ğŸ”¥")
        print("Error:", e)
        import traceback
        traceback.print_exc()

        return "í˜„ì¬ AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."




def preload_qwen():
    """
    Django ì„œë²„ ì‹œì‘ ì‹œ ë¯¸ë¦¬ í•œ ë²ˆ í˜¸ì¶œí•´ì„œ
    ì²« ì§ˆë¬¸ì—ì„œ ë¡œë”© ë”œë ˆì´ê°€ ì•ˆ ìƒê¸°ê²Œ í•˜ëŠ” ìš©ë„.
    """
    _load_model()
